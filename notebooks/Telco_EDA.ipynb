{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the data\n",
    "pd.set_option('max_column', None)\n",
    "pd.set_option('display.float_format', repr)\n",
    "#pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "original_data=pd.read_csv(\"D:/Stella/Documents/10_Academy/Week_1/Week-1/data/Week1_challenge_data.csv\")\n",
    "data=original_data.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names in the dataset\n",
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of data points\n",
    "print(f\" There are {data.shape[0]} rows and {data.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total values missing in the dataset as a numerical value and as a percentage\n",
    "# how many missing values exist or better still what is the % of missing values in the dataset?\n",
    "def percent_missing(df):\n",
    "\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    print(\"The Telco dataset contains\", totalMissing,\" missing values or\", round(((totalMissing/totalCells) * 100), 2), \"%\", \"of the dataset.\")\n",
    "\n",
    "percent_missing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total percent of missing values per column\n",
    "def missing_per_column(df):\n",
    "    print(round((df.isna().sum()*100)/len(df), 2))\n",
    "missing_per_column(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data\n",
    "Missing values in the categorical columns will be replaced using the mode while the mean will be used for quantitative columns.\n",
    "The histogram below shows some columns are skewed that may make the mean unsuitable for handling missing values.\n",
    "However, the mean is used because the dataset will be normalized to make the values follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def exploratory_hist(df):\n",
    "    df.hist(figsize=(15,15), layout=(11,5))\n",
    "plt.show()\n",
    "exploratory_hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots = {i: px.histogram(data, x=i,) for i in data.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots['Bearer Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values using the mean and mode\n",
    "def impute(df):\n",
    "    cat_list=['Bearer Id','Start','End','IMSI','MSISDN/Number','IMEI','Last Location Name','Handset Manufacturer','Handset Type']\n",
    "    for i in df.columns:\n",
    "        if i not in cat_list:\n",
    "            df[i]=df[i].fillna(df[i].mean())\n",
    "        else:\n",
    "            df[i]=df[i].fillna(df[i].value_counts().index[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalizer(df):\n",
    "    df_new=df.drop(['Bearer Id','Start','End','IMSI','MSISDN/Number','IMEI','Last Location Name','Handset Manufacturer','Handset Type'],axis=1)\n",
    "    norm = Normalizer()\n",
    "    #norm=RobustScaler()\n",
    "    # normalize the data with boxcox\n",
    "    normalized_data = norm.fit_transform(df_new)\n",
    "    normalized_df=pd.DataFrame(normalized_data,columns = df_new.columns)\n",
    "    cols_to_use = df.columns.difference(normalized_df.columns)\n",
    "    final_df=pd.merge(df[cols_to_use],normalized_df, left_index=True, right_index=True, how='outer')\n",
    "    #final_df=pd.concat([df,normalized_df],axis=1)\n",
    "    return final_df\n",
    "    #final_df.hist(figsize=(15,15), layout=(11,5))\n",
    "    \n",
    "normalizer(impute(data)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_hist(normalizer(impute(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert start and end to date time\n",
    "def date_time(df):\n",
    "    df['Start']=pd.to_datetime(df['Start'], infer_datetime_format=True)\n",
    "    df['End']=pd.to_datetime(df['End'], infer_datetime_format=True)\n",
    "    return df\n",
    "date_time(impute(data)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "(impute(data)).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "missing_per_column(impute(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_data=impute(data)\n",
    "#clean_data=normalizer(impute(data))\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[clean_data[\"MSISDN/Number\"] == 33664962239.0]['Bearer Id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered= clean_data[(clean_data[\"MSISDN/Number\"] == 33664962239)]\n",
    "xDR_Count=df_filtered['Bearer Id'].nunique()\n",
    "xDR_Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate by xDR sessions per user\n",
    "sessions_per_user=clean_data.groupby([\"MSISDN/Number\"],as_index=True)['Bearer Id'].count().reset_index().sort_values(['Bearer Id'],ascending=False).head(5)\n",
    "sessions_per_user                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggregate per user on session duration\n",
    "duration_per_user=clean_data.groupby([\"MSISDN/Number\"],as_index=True)['Dur. (ms)'].sum().reset_index().sort_values(['Dur. (ms)'],ascending=False).head(5)\n",
    "duration_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggregate by Total DL and UL per user \n",
    "data_per_user=clean_data.groupby([\"MSISDN/Number\"],as_index=True).agg(Total_UL=(\"Total UL (Bytes)\", sum),\n",
    "                                        Total_DL=(\"Total DL (Bytes)\", sum)).reset_index().sort_values([\"Total_UL\",\n",
    "                                        \"Total_DL\"],ascending=False).head(5)\n",
    "data_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data for applications\n",
    "app_data=clean_data[['Bearer Id',\"MSISDN/Number\",\"Youtube DL (Bytes)\",\"Youtube UL (Bytes)\",\"Social Media DL (Bytes)\",\"Social Media UL (Bytes)\",\n",
    "                   \"Google DL (Bytes)\",\"Google UL (Bytes)\",\"Email DL (Bytes)\",\"Email UL (Bytes)\",\n",
    "                   \"Netflix DL (Bytes)\",\"Netflix UL (Bytes)\",\"Gaming DL (Bytes)\",\"Gaming UL (Bytes)\",\n",
    "                   \"Other DL (Bytes)\",\"Other UL (Bytes)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app_data[\"Youtube\"]=clean_data[\"Youtube DL (Bytes)\"]+clean_data[\"Youtube UL (Bytes)\"]\n",
    "app_data[\"Social Media\"]=clean_data[\"Social Media DL (Bytes)\"]+clean_data[\"Social Media UL (Bytes)\"] \n",
    "app_data[\"Google\"]=clean_data[\"Google DL (Bytes)\"]+clean_data[\"Google UL (Bytes)\"]\n",
    "app_data[\"Email\"]=clean_data[\"Email DL (Bytes)\"]+clean_data[\"Email UL (Bytes)\"]\n",
    "app_data[\"Netflix\"]=clean_data[\"Netflix DL (Bytes)\"]+clean_data[\"Netflix UL (Bytes)\"]\n",
    "app_data[\"Gaming\"]=clean_data[\"Gaming DL (Bytes)\"]+clean_data[\"Gaming UL (Bytes)\"]\n",
    "app_data[\"Other\"]=clean_data[\"Other DL (Bytes)\"]+clean_data[\"Other UL (Bytes)\"]\n",
    "#app_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UL_DL_per_app=clean_data.groupby([\"MSISDN/Number\"]).agg(Youtube_DL=(\"Youtube DL (Bytes)\",sum),\n",
    "                                         Youtube_UL=(\"Youtube UL (Bytes)\",sum),\n",
    "                                         Social_Media_DL=(\"Social Media DL (Bytes)\",sum),\n",
    "                                         Social_Media_UL=(\"Social Media UL (Bytes)\",sum),                           \n",
    "                                         Google_DL=(\"Google DL (Bytes)\",sum),\n",
    "                                         Google_UL=(\"Google UL (Bytes)\",sum),                          \n",
    "                                         Email_DL=(\"Email DL (Bytes)\",sum),\n",
    "                                         Email_UL=(\"Email UL (Bytes)\",sum),\n",
    "                                         Netflix_DL=(\"Netflix DL (Bytes)\",sum),\n",
    "                                         Netflix_UL=(\"Netflix UL (Bytes)\",sum),\n",
    "                                         Gaming_DL=(\"Gaming DL (Bytes)\",sum),\n",
    "                                         Gaming_UL=(\"Gaming UL (Bytes)\",sum),\n",
    "                                         Other_DL=(\"Other DL (Bytes)\",sum),\n",
    "                                         Other_UL=(\"Other UL (Bytes)\",sum)).head(6)\n",
    "UL_DL_per_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate by the total data volume (in Bytes) during this session for each application for each user\n",
    "data_per_app=app_data.groupby([\"MSISDN/Number\"]).agg(Youtube=(\"Youtube\",sum),\n",
    "                                         Social_Media=(\"Social Media\",sum),\n",
    "                                         Google=(\"Google\",sum),\n",
    "                                         Email=(\"Email\",sum),\n",
    "                                         Netflix=(\"Netflix\",sum),\n",
    "                                         Gaming=(\"Gaming\",sum),\n",
    "                                         Other=(\"Other\",sum)).head(6)\n",
    "data_per_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the mean, median and standard deviation of the aggregated datasets\n",
    "pd.DataFrame([[\"xDR_seesion_count\",sessions_per_user[\"Bearer Id\"].mean(),sessions_per_user[\"Bearer Id\"].median(),\n",
    "               sessions_per_user[\"Bearer Id\"].std()],[\"Duration_per_user\",duration_per_user[\"Dur. (ms)\"].mean(),\n",
    "            duration_per_user[\"Dur. (ms)\"].median(),duration_per_user[\"Dur. (ms)\"].std()],\n",
    "              [\"Upload_Data\",data_per_user[\"Total_UL\"].mean(),data_per_user[\"Total_UL\"].median(),\n",
    "               data_per_user[\"Total_UL\"].std()],\n",
    "             [\"Download_Data\",data_per_user[\"Total_DL\"].mean(),data_per_user[\"Total_DL\"].median(),\n",
    "               data_per_user[\"Total_DL\"].std()],\n",
    "             [\"Youtube Data\",data_per_app[\"Youtube\"].mean(),data_per_app[\"Youtube\"].median(),data_per_app[\"Youtube\"].std()],\n",
    "             [\"Social Media Data\",data_per_app[\"Social_Media\"].mean(),data_per_app[\"Social_Media\"].median(),data_per_app[\"Social_Media\"].std()],\n",
    "             [\"Google Data\",data_per_app[\"Google\"].mean(),data_per_app[\"Google\"].median(),data_per_app[\"Google\"].std()],\n",
    "             [\"Email Data\",data_per_app[\"Email\"].mean(),data_per_app[\"Email\"].median(),data_per_app[\"Email\"].std()],\n",
    "             [\"Netflix Data\",data_per_app[\"Netflix\"].mean(),data_per_app[\"Netflix\"].median(),data_per_app[\"Netflix\"].std()],\n",
    "             [\"Gaming Data\",data_per_app[\"Gaming\"].mean(),data_per_app[\"Gaming\"].median(),data_per_app[\"Gaming\"].std()],\n",
    "             [\"Other Data\",data_per_app[\"Other\"].mean(),data_per_app[\"Other\"].median(),data_per_app[\"Other\"].std()]],\n",
    "              columns=['variable','mean','median','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([[\"xDR_seesion_count\",sessions_per_user[\"Bearer Id\"].mean(),sessions_per_user[\"Bearer Id\"].min(),\n",
    "#                sessions_per_user[\"Bearer Id\"].max(),sessions_per_user[\"Bearer Id\"].median(),sessions_per_user[\"Bearer Id\"].quantile(0.25),\n",
    "#                sessions_per_user[\"Bearer Id\"].quantile(0.75)],\n",
    "#               [\"Duration_per_user\",duration_per_user[\"Dur. (ms)\"].mean(),\n",
    "#             duration_per_user[\"Dur. (ms)\"].min(),duration_per_user[\"Dur. (ms)\"].max(),duration_per_user[\"Dur. (ms)\"].median(),\n",
    "#                duration_per_user[\"Dur. (ms)\"].quantile(0.25),duration_per_user[\"Dur. (ms)\"].quantile(0.75),],\n",
    "#               [\"Upload_Data\",data_per_user[\"Total_UL\"].mean(),data_per_user[\"Total_UL\"].min(),\n",
    "#                data_per_user[\"Total_UL\"].max(),data_per_user[\"Total_UL\"].median(),data_per_user[\"Total_UL\"].quantile(0.25),\n",
    "#                data_per_user[\"Total_UL\"].quantile(0.75)],\n",
    "#              [\"Download_Data\",data_per_user[\"Total_DL\"].mean(),data_per_user[\"Total_DL\"].min(),\n",
    "#                data_per_user[\"Total_DL\"].max(),data_per_user[\"Total_DL\"].median(),data_per_user[\"Total_DL\"].quantile(0.25),data_per_user[\"Total_DL\"].quantile(0.75)],\n",
    "#              [\"Youtube Data\",data_per_app[\"Youtube\"].mean(),data_per_app[\"Youtube\"].min(),data_per_app[\"Youtube\"].max(),data_per_app[\"Youtube\"].median(),\n",
    "#              data_per_app[\"Youtube\"].quantile(0.25),data_per_app[\"Youtube\"].quantile(0.75)],\n",
    "#              [\"Social Media Data\",data_per_app[\"Social_Media\"].mean(),data_per_app[\"Social_Media\"].min(),data_per_app[\"Social_Media\"].max(),\n",
    "#              data_per_app[\"Social_Media\"].median(),data_per_app[\"Social_Media\"].quantile(0.25),data_per_app[\"Social_Media\"].quantile(0.75)],\n",
    "#              [\"Google Data\",data_per_app[\"Google\"].mean(),data_per_app[\"Google\"].min(),data_per_app[\"Google\"].max(),\n",
    "#              data_per_app[\"Google\"].median(),data_per_app[\"Google\"].quantile(0.25),data_per_app[\"Google\"].quantile(0.75)],\n",
    "#              [\"Email Data\",data_per_app[\"Email\"].mean(),data_per_app[\"Email\"].min(),data_per_app[\"Email\"].max(),\n",
    "#              data_per_app[\"Email\"].median(),data_per_app[\"Email\"].quantile(0.25),data_per_app[\"Email\"].quantile(0.75)],\n",
    "#              [\"Netflix Data\",data_per_app[\"Netflix\"].mean(),data_per_app[\"Netflix\"].median(),data_per_app[\"Netflix\"].std(),\n",
    "#              data_per_app[\"Netflix\"].median(),data_per_app[\"Netflix\"].quantile(0.25),data_per_app[\"Netflix\"].quantile(0.75)],\n",
    "#              [\"Gaming Data\",data_per_app[\"Gaming\"].mean(),data_per_app[\"Gaming\"].min(),data_per_app[\"Gaming\"].max(),\n",
    "#              data_per_app[\"Gaming\"].median(),data_per_app[\"Gaming\"].quantile(0.25),data_per_app[\"Gaming\"].quantile(0.75)],\n",
    "#              [\"Other Data\",data_per_app[\"Other\"].mean(),data_per_app[\"Other\"].min(),data_per_app[\"Other\"].max(),\n",
    "#              data_per_app[\"Other\"].median(),data_per_app[\"Other\"].quantile(0.25),data_per_app[\"Other\"].quantile(0.75)]],\n",
    "#               columns=['variable','mean','min','max','2Q','1Q','3Q'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_filtered =  data_per_app[ data_per_app.index == 33601001754.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(app_filtered.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#px.pie(app_filtered, values=np.array(app_filtered.iloc[0]), names=app_filtered.columns, title='Data Used Per Application')\n",
    "px.pie(app_filtered, values=np.array(app_filtered.iloc[0]), names=app_filtered.columns, title='Data Used Per Application')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_per_app.T.plot.pie(subplots=True, figsize=(11, 6))\n",
    "#Pie chart showing data usage for six random users\n",
    "wp = { 'linewidth' : 1, 'edgecolor' : \"green\" }\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "for i, (name, row) in enumerate(data_per_app.iterrows()):\n",
    "    ax = plt.subplot(3,2, i+1)\n",
    "    ax.set_title(row.name)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.pie(row, labels=row.index,\n",
    "                                  shadow = True,autopct='%1.1f%%',\n",
    "                                  wedgeprops = wp,\n",
    "                                  textprops = dict(color =\"black\"))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pie chart showing which application uses the most data\n",
    "explode = [0.8, 0.8, 0.5, 0.5, 0.8,0.2,0.2]\n",
    "colors = ['tab:blue', 'tab:cyan', 'tab:gray', 'tab:orange', 'tab:red', 'tab:green','tab:brown']\n",
    "\n",
    "data_per_app[data_per_app.columns[0:]].sum().plot.pie(shadow = True,autopct='%1.1f%%', colors = colors, explode = explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar chart comparing Total download and Total upload\n",
    "data_per_user[data_per_user.columns[1:]].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot to show position of outliers in count of sessions\n",
    "sns.boxplot(sessions_per_user['Bearer Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of duration spent in the sessions\n",
    "plt.hist(duration_per_user['Dur. (ms)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bivariate Analysis using stacked bar plot\n",
    "axis = UL_DL_per_app[[\"Other_DL\", \"Other_UL\"]].plot(kind=\"bar\", stacked=True)\n",
    "fig = axis.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix indicates whether any relationship exists between the variables and the strength of that relationship.\n",
    "In this case we would be looking at whether an increase in the data used in one application affects data used in another application. For example the relationship between data used in google versus data used in social media is a strong negative one. That means an increase in data used in google means less data will be used on social media.\n",
    "The heatmap gives a good visual representation of the strength of the relationships, the lighter the colour the more positive the relationship and the darker the colour the more negative the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation analysis\n",
    "corr = data_per_app.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=original_data.copy()\n",
    "clean_data2=impute(data2)\n",
    "#clean_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aggregating by number of sessions\n",
    "def count_sessions(df):\n",
    "    sessions_per_user=pd.DataFrame(df.groupby([\"MSISDN/Number\"],as_index=True)['Bearer Id'].count()).sort_values(['Bearer Id'],ascending=False)\n",
    "    return sessions_per_user \n",
    "count_sessions(clean_data2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def total_duration(df):\n",
    "    duration_per_user=pd.DataFrame(df.groupby([\"MSISDN/Number\"],as_index=True)['Dur. (ms)'].sum()).sort_values(['Dur. (ms)'],ascending=False)\n",
    "    return duration_per_user\n",
    "total_duration(clean_data2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_data(df):\n",
    "    df['Total Data Used']=df[\"Total DL (Bytes)\"]+df[\"Total UL (Bytes)\"]\n",
    "    data_per_user=pd.DataFrame(df.groupby([\"MSISDN/Number\"],as_index=True)['Total Data Used'].sum()).sort_values(['Total Data Used'],ascending=False)\n",
    "    return data_per_user\n",
    "total_data(clean_data2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Each Metric\n",
    "Use the Standard Scaler that ensures the data have a mean of 0 and variance of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of all engagement metrics\n",
    "\n",
    "engagement_df=reduce(lambda x,y: pd.merge(x,y, on='MSISDN/Number', how='outer'), [total_duration(clean_data2), total_data(clean_data2), count_sessions(clean_data2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def norm_df(df):\n",
    "#     norm = Normalizer()\n",
    "#     normalized_data = pd.DataFrame(norm.fit_transform(df))\n",
    "#     return normalized_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "K-Means is an unsupervised ML algorithm the finds clusters within the dataset. We begin with 3 initial clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(engagement_df)\n",
    "\n",
    "# statistics of scaled data\n",
    "normalized_data=pd.DataFrame(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the kmeans function with initialization as k-means++\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++')\n",
    "\n",
    "# fitting the k means algorithm on scaled data\n",
    "kmeans.fit(data_scaled)\n",
    "\n",
    "#predict the labels of clusters.\n",
    "label = kmeans.fit_predict(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting multiple k-means algorithms and storing the values in an empty list\n",
    "SSE = []\n",
    "for cluster in range(1,20):\n",
    "    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')\n",
    "    kmeans.fit(data_scaled)\n",
    "    SSE.append(kmeans.inertia_)\n",
    "\n",
    "# converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting unique labels\n",
    " \n",
    "u_labels = np.unique(label)\n",
    " \n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels:\n",
    "    plt.scatter(data_scaled[label == i , 0] , data_scaled[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0 and 2 are overlapping on the plot therefore we use a scree plot to determine the appropriate number of clusters.\n",
    "Using the elbow method, the appropriate number of clusters is between two and five. Using two major two major clusters of people in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means clustering with two classes\n",
    "# defining the kmeans function with initialization as k-means++\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++')\n",
    "\n",
    "# fitting the k means algorithm on scaled data\n",
    "kmeans.fit(data_scaled)\n",
    "\n",
    "#predict the labels of clusters.\n",
    "label = kmeans.fit_predict(data_scaled)\n",
    "#Getting unique labels\n",
    " \n",
    "u_labels = np.unique(label)\n",
    " \n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels:\n",
    "    plt.scatter(data_scaled[label == i , 0] , data_scaled[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('total_duration')\n",
    "plt.ylabel('total_data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster zero have low usage of both the first and second variable while cluster 1 have high usage of both variables in the dataset.(Referring to the first two variables in the dataset,duration and data used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the clusters to the dataset\n",
    "engagement_df['clusters'] = label.tolist()\n",
    "engagement_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variables in the dataset, a 3D plot would be needed to interpret the results, for ease of interpretation reduce the dimension of the data to two by PCA. According to PCA there are many users with low usage of both components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run PCA on the data and reduce the dimensions in pca_num_components dimensions\n",
    "\n",
    "# Create a PCA model to reduce our data to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data_scaled)\n",
    "\n",
    "# Transform the scaled data to the new PCA space\n",
    "X_reduced = pd.DataFrame(pca.transform(data_scaled),columns=['pca1','pca2'])\n",
    "sns.scatterplot(x=\"pca1\", y=\"pca2\", data=X_reduced)\n",
    "plt.title('K-means Clustering with 2 dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_df.groupby('clusters').agg(['min', 'max', 'mean', 'sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Data per Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_agg(df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:\n",
    "    \n",
    "    new_df = df.groupby(agg_column)[col_name].agg(agg_metric).reset_index(name=col_name).\\\n",
    "                        sort_values(by=col_name, ascending=order)[:top]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Youtube\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Social_Media top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Social Media\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Google\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Email top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Email\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Netflix top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Netflix\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaming top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Gaming\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other top 10 users\n",
    "find_agg(app_data,\"MSISDN/Number\",\"sum\",\"Other\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the top 3 most used applications using appropriate charts\n",
    "data_per_app.sum().nlargest(3).plot(kind='bar')\n",
    "plt.title('Top 3 Apps')\n",
    "plt.xlabel('App')\n",
    "plt.ylabel('Data usage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier detection and replacement by mean\n",
    "def outlier(df):\n",
    "    column_name=['Avg RTT DL (ms)','Avg RTT UL (ms)','TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',\n",
    "                'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',\n",
    "                'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',\n",
    "                '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',\n",
    "                'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',\n",
    "                '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)']\n",
    "    for i in column_name:\n",
    "        upper_quartile=df[i].quantile(0.75)\n",
    "        lower_quartile=df[i].quantile(0.25)\n",
    "        df[i]=np.where(df[i]>upper_quartile,df[i].mean(),np.where(df[i]<lower_quartile,df[i].mean(),df[i]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data3=outlier(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(clean_data['DL TP < 50 Kbps (%)'],data=clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two boxplots show that outliers are no longer present in the dataset.\n",
    "sns.boxplot(clean_data3['DL TP < 50 Kbps (%)'],data=clean_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate total TCP,RTT and Throughput\n",
    "clean_data3[\"Total TCP\"]=clean_data3['TCP DL Retrans. Vol (Bytes)']+ clean_data3['TCP UL Retrans. Vol (Bytes)']+clean_data3['TCP DL Retrans. Vol (Bytes)']+ clean_data3['TCP UL Retrans. Vol (Bytes)']\n",
    "clean_data3[\"Total RTT\"]=clean_data3['Avg RTT DL (ms)']+clean_data3['Avg RTT UL (ms)']\n",
    "clean_data3[\"Total Throughput\"]=clean_data3['DL TP < 50 Kbps (%)']+clean_data3['50 Kbps < DL TP < 250 Kbps (%)']+clean_data3['250 Kbps < DL TP < 1 Mbps (%)']+clean_data3['DL TP > 1 Mbps (%)']+clean_data3['UL TP < 10 Kbps (%)']+clean_data3['10 Kbps < UL TP < 50 Kbps (%)']+clean_data3['50 Kbps < UL TP < 300 Kbps (%)']+clean_data3['UL TP > 300 Kbps (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average TCP per customer\n",
    "#find_agg(clean_data3,\"MSISDN/Number\",\"mean\",\"Total TCP\",10)\n",
    "avg_TCP=clean_data3.groupby('MSISDN/Number')['Total TCP'].mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average RTT per customer\n",
    "#find_agg(clean_data3,\"MSISDN/Number\",\"mean\",\"Total RTT\",10)\n",
    "avg_RTT=clean_data3.groupby('MSISDN/Number')['Total RTT'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Throughput per customer\n",
    "#find_agg(clean_data3,\"MSISDN/Number\",\"mean\",\"Total Throughput\",10)\n",
    "avg_throughput=clean_data3.groupby('MSISDN/Number')['Total Throughput'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Handsets owned by an individual\n",
    "#find_agg(clean_data3,\"MSISDN/Number\",\"count\",\"Handset Type\",10)\n",
    "count_handsets=clean_data3.groupby(['MSISDN/Number','Handset Type'])['Handset Type'].count().sort_values(ascending=False)\n",
    "count_handsets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Ten Handsets used by manufacturer\n",
    "#f = {Handset_Manufacturer_Count='Handset Manufacturer': 'count'}\n",
    "clean_data3.groupby(['Handset Manufacturer','Handset Type']).agg(Handset_Manufacturer_Count=('Handset Manufacturer', 'count')).sort_values(['Handset Manufacturer','Handset_Manufacturer_Count'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Ten Handsets used by customers\n",
    "#f = {Handset_Manufacturer_Count='Handset Manufacturer': 'count'}\n",
    "clean_data3.groupby(['MSISDN/Number','Handset Type']).agg(Handset_Type_Count=('Handset Type', 'count')).sort_values(['Handset_Manufacturer_Count'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Top 3 Handset manufacturers\n",
    "find_agg(clean_data3,\"Handset Manufacturer\",\"count\",\"Handset Type\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 handsets per top 3 handset manufacturer\n",
    "options = ['Apple', 'Samsung', 'Huawei']\n",
    "  \n",
    "# selecting rows based on condition\n",
    "top_3_df = clean_data3.loc[clean_data3['Handset Manufacturer'].isin(options)]\n",
    "top_3_df.groupby(['Handset Manufacturer','Handset Type']).agg(Handset_Manufacturer_Count=('Handset Manufacturer', 'count')).sort_values(['Handset Manufacturer','Handset_Manufacturer_Count'],ascending=False).groupby('Handset Manufacturer').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most frequent TCP\n",
    "clean_data3['Total TCP'].value_counts().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 RTT values\n",
    "clean_data3['Total RTT'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom 10 RTT values\n",
    "clean_data3['Total RTT'].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Top 10 Throughput values\n",
    "clean_data3['Total Throughput'].value_counts().head(10)\n",
    "clean_data3['Total Throughput'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom 10 Throughput values\n",
    "clean_data3['Total Throughput'].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram can be used to show distributionns of the experience metrics\n",
    "Total throughput is left skewed so most handsets have an average throughput of 200\n",
    "Most handsets also have a high average TCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data3.groupby('Handset Manufacturer')['Total Throughput'].mean().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data3.groupby('Handset Manufacturer')['Total TCP'].mean().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "The K-Means Cluster Plot is based on average RTT and average TCP. The people in cluster 0 tend to have a high RTT and a high TCP, the people in cluster 1 have a high RTT while the people in cluster 2 have a high TCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experience_df=reduce(lambda x,y: pd.merge(x,y, on='MSISDN/Number', how='outer'), [avg_RTT,avg_TCP, avg_throughput, count_handsets])\n",
    "experience_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_experience = scaler.fit_transform(experience_df)\n",
    "# defining the kmeans function with initialization as k-means++\n",
    "kmeans_exp = KMeans(n_clusters=3, init='k-means++')\n",
    "\n",
    "# fitting the k means algorithm on scaled data\n",
    "kmeans_exp.fit(scaled_experience)\n",
    "\n",
    "#predict the labels of clusters.\n",
    "label_exp = kmeans_exp.fit_predict(scaled_experience)\n",
    "\n",
    "u_labels_exp = np.unique(label_exp)\n",
    " \n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels_exp:\n",
    "    plt.scatter(scaled_experience[label_exp == i , 0] , scaled_experience[label_exp == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('Average RTT')\n",
    "plt.ylabel('Average TCP')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "engagement_score=euclidean_distances(data_scaled, centroids)\n",
    "engagement_df['engagement_score']=[item[0] for item in engagement_score]\n",
    "engagement_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_exp = kmeans_exp.cluster_centers_\n",
    "experience_score=euclidean_distances(scaled_experience, centroids_exp)\n",
    "experience_df['experience_score']=[item[0] for item in experience_score]\n",
    "experience_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df=pd.merge(engagement_df[[\"engagement_score\"]], experience_df[[\"experience_score\"]], on=\"MSISDN/Number\", how='outer')\n",
    "score_df=score_df[~score_df.index.duplicated(keep='first')]\n",
    "score_df['satisfaction_score'] = score_df.mean(axis=1)\n",
    "score_df=score_df.sort_values(['satisfaction_score'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Prediction using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df=reduce(lambda x,y: pd.merge(x,y, on='MSISDN/Number', how='outer'), [engagement_df, experience_df, score_df[['satisfaction_score']]]).drop([\"engagement_score\",\"experience_score\",\"clusters\"],axis=1)\n",
    "reg_df=reg_df[~reg_df.index.duplicated(keep='first')]\n",
    "reg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reg_df.drop(['satisfaction_score'], axis=1), reg_df['satisfaction_score'], test_size = 0.20, random_state=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating scaler scale var.\n",
    "norm = MinMaxScaler()\n",
    "# fit the scale\n",
    "norm_fit = norm.fit(X_train)\n",
    "scal_xtrain = norm_fit.transform(X_train)\n",
    "\n",
    "# transformation of testing data\n",
    "scal_xtest = norm_fit.transform(X_test)\n",
    "# create model variable\n",
    "rnd = RandomForestRegressor(n_estimators = 100, random_state = 100)\n",
    "  \n",
    "# fit the model\n",
    "fit_rnd = rnd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "x_predict = list(rnd.predict(X_test))\n",
    "# Calculate the absolute errors\n",
    "errors = abs(x_predict - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': x_predict}).sort_values(['Actual'],ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_experience = scaler.fit_transform(score_df)\n",
    "# defining the kmeans function with initialization as k-means++\n",
    "kmeans_score = KMeans(n_clusters=2, init='k-means++')\n",
    "\n",
    "# fitting the k means algorithm on scaled data\n",
    "kmeans_score.fit(score_experience)\n",
    "\n",
    "#predict the labels of clusters.\n",
    "label_score = kmeans_score.fit_predict(score_experience)\n",
    "\n",
    "u_labels_score = np.unique(label_score)\n",
    " \n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels_score:\n",
    "    plt.scatter(score_experience[label_score == i , 0] , score_experience[label_score == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('Engagement Score')\n",
    "plt.ylabel('Experience Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the average satisfaction & experience score per cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df['clusters'] = label_score.tolist()\n",
    "score_df.groupby('clusters').agg(['mean'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
